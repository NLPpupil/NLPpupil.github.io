---
layout: post
title:  "维特比算法"
date:   2018-10-20
categories: 自然语言处理
---


本文介绍维特比算法在隐马尔可夫模型解码问题中的应用。HMM的解码问题，也叫预测问题，指的是给定HMM参数和观测序列，求最有可能的状态序列，或称最优路径。

设状态数为$N$，状态集合为$Q$，观测序列长度为$T$。如果采用穷举法，即求出所有状态序列的概率，取最大的那条状态序列，那么每一个时刻都有$N$种可能，一共有$N^T$个状态序列。计算机不喜欢指数级复杂度的算法，neither do 程序员。


但行好事，莫问前程。我们只要计算每个时刻每个状态可以达到的最大概率，最终就可以求出最大状态序列的概率和相应的状态序列。

维特比变量定义为HMM在时刻$t$处于状态$i$且前$t-1$个状态是使得状态$j$概率最大的路径的概率。即，
$$
v_t(i) = \max_{q_1,...,q_{(t-1)}} P(q_1,q_2,...,q_{(t-1)},i,o_1,o_2,...,o_t \mid \lambda)
$$
一个configuration是特定的一个序列的状态和相应的一个序列的观测同时发生的事件。维特比变量是时刻$t$时最后一个状态是$i$的所有confugurations中最大概率的那个congiguration的概率。每个时刻有$N$个维特比变量，$T$时刻的$N$个维特比变量中最大的维特比变量就是包含最可能的状态序列的configuration的概率。

所以从$t=1$开始，每个时刻根据定义递推地计算$N$个维特比变量，就可以得到$T$时刻最大的维特比变量。采用李航《统计学习方法》中的定义，一个HMM由三要素组成：转移矩阵$A$，发射矩阵$B$和状态初始概率$\pi$。如果先解决维特比变量递推计算，算法是这样的：


-
**维特比变量递推算法**
<br>
输入：模型$\lambda = (A,B,\pi)$和观测序列$O = (o_1,o_2, ... ,o_T)$
<br>
输出：最优路径对应的configuration的概率

1.初始化
<br>
$t=1$时，计算每个状态和$o_1$在$\lambda$下同时发生的概率，即$t=1$时的$N$个维特比变量。
每个维特比变量等于初始状态概率乘以相应的发射概率。


$$
v_t(i) = \pi[i]\times emission[i][o_1], i \in Q
$$

2.递推
<br>
对$t=2,3,...,T$：
<br>
$t-1$时的某个confuguration转移一次，发射一次，就变成了$t$时刻的某个configuration。也就是说$v_t(i)$对应的configuration由某个前一时刻的configuration转移发射而来，而前一时刻的configuration只有$N$类（以每个状态为结尾的归为一类），每一类最好的configuration的概率已经由相应的维特比变量算好了。也就是说$v_t(i)$是前一时刻的$N$个维特比变量每一个乘以转移到$i$的概率，再乘以$i$到$o_t$的发射概率的$N$个待选值中最大的那个值。
$$
v_t(i) = \max v_{t-1}(j)\times transition[j][i]\times emission[i][o_t], i,j \in Q
$$


3.终止
<br>
返回
 
$$
\max v_T(i), i \in Q
$$

-

有了维特比变量递推算法，最优路径也就呼之欲出了，只需要在计算每个维特比变量的时候，顺便存储它是由哪一个状态转移而来。所谓的维特比算法，就是在维特比变量递推算法基础上，加一个backpointer变量存储每一时刻每一状态是由哪个状态转移而来达到最大概率。相比与《统计学习方法》的算法描述，我更喜欢[《Speech and Language Processing》](https://web.stanford.edu/~jurafsky/slp3/)的描述：

3.<img src="https://nlppupil.github.io/images/viterbi" alt="BP" style="width:200px;height:380px;">









