---
layout: post
title:  "生成式对抗网络原始版本"
date:   2018-06-14
categories: 深度学习 机器学习 概率图
---

本文介绍生成式对抗网络的原始版本，即Goodfellow在2014年提出的
[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)，截止目前已被引用3408次。

#### 性质
GAN是一种生成式模型。生成式模型从来自真实分布$p_{data}$的训练集学习一个此分布的估计概率分布$p_{model}$。生成式模型直接对联合概率进行建模，判别式模型则是对条件分布进行建模。

GAN也是一种概率图模型。概率图模型，又叫结构化概率模型，它是一种用图描述概率分布的方式。图的节点表示随机变量，连线表示随机变量之间的关系。概率图的优点是大大减少了模型的计算复杂度。GAN是概率图的一个分支——有向图模型，有向图模型又叫信念网(belief network)或贝叶斯网。在有向图模型里，$a$节点指向$b$节点定义了一个条件概率分布$P(b \mid a)$。GAN的图结构如下：

<img src="https://nlppupil.github.io/images/gan_graph.png" alt="BP" style="width:100px;height:190px;">


#### 基本结构
生成器努力生成跟训练数据相同分布的样例，判别器判别一个样例的真假，二者在相互为难中共同提高。这就是对抗的含义。

生成器是一个函数$G$，对于它的输入$z$和参数$\theta^{(G)}$都是可微的。$z$来自一个任意的固定的简单分布。$G(z)$产生一个来自$p_{model}$的样例。判别器是一个对它的参数$\theta^{(D)}$可微的函数$D$。$D(x)$表示样例$x$来自真实分布的概率。

二者的损失函数都通过双方的参数定义。判别器想要最小化$J^{(D)}(\theta^{(D)},\theta^{(G)})$但是只能控制$\theta^{(D)}$。生成器想要最小化$J^{(G)}(\theta^{(D)},\theta^{(G)})$但是只能控制$\theta^{(G)}$。

因为二者的损失都依赖于对方的参数但又只能控制自己的参数，这种场景更像一个游戏而不是优化问题。优化问题的解是一个极小值，游戏的解是一个纳什均衡。

什么是纳什均衡？在博弈论里，一个玩家(player)的策略(strategy)在所有可能发生情况下的一套完整行动计划，在这里一个行动的产出(outcome)不仅取决于玩家自己的行动， 还取决于其他玩家的行动。一个策略是一个完整的算法，它指导玩家在所有情况下应采取的行动。在GAN中，玩家有生成器和判别器。策略就是相应的参数。生成器的产出是一个样本，判别器的产出是一个概率。策略组合(strategy profile, or strategy combination)是一个集合的策略，它指定了一个游戏里所有的行动。一个策略组合必须包含每个玩家的一个策略且一个玩家只能有一个策略。一个策略组合是纳什均衡，如果玩家单方面改变它的策略不能做得更好。意思是，如果每个玩家虽然知道其他玩家的策略，但不能改变其他玩家的策略且假设其他玩家的策略不变，那么它无法通过改变自己的策略让自己在游戏中更加获益。

在GAN中，纳什均衡是一个二元组$(\theta^{(D)},\theta^{(G)})$，它是$J^{(D)}$相对于$\theta^{(D)}$的局部最小值，也是$J^{(G)}$相对于$\theta^{(G)}$的局部最小值。即，生成器和判别器都知道对方的参数，且视对方的参数为固定，如果双方都不能通过调整自己的参数使自己的损失继续下降，就达到了纳什均衡。

#### KL差异和JS差异
KL差异(Kullback–Leibler divergence)，也叫KL距离，衡量了两个概率分布之间的差异。

$$
\begin{equation}
D_{KL}(p || q) = \int_{x} p(x)log\frac{p(x)}{q(x)}  dx = \sum_x p(x)log\frac{p(x)}{q(x)}
\end{equation}
$$

当$D_{KL}$最小等于$0$时，$p=q$。KL差异是非对称的。

JS差异(Jensen–Shannon Divergence)是另一个衡量两个概率分布的差异的计算方法，它是对称的。

$$
\begin{equation}
D_{JS}(p || q) = \frac{1}{2}D_{KL}(p ||  \frac{p+q}{2}) + \frac{1}{2}D_{KL}(q ||  \frac{p+q}{2})
\end{equation}
$$

#### 交叉熵
$$
\begin{equation}
CrossEnt(p,q) = \mathbb{E}_p[-\log q] = -\sum_x p(x) \log q(x) = D_{KL}(p || q) - \sum_x p(x) \log p(x)
\end{equation}
$$





#### 极大似然估计
似然是模型分配给训练数据的概率$\prod_x p_{model}(x,\theta)$，极大似然估计就是找到使得似然最大的参数$\theta^*$，实际应用中一般是最大化对数似然：

$$
\theta^* = \arg \max_\theta \sum_x \log p_{model}(x)
$$



#### 判别器的损失函数
将上面的概念都理清之后，就可以定义判别器的损失函数了。
<br>
判别器采用极大似然估计来学习参数，将似然加上对数再加上负号，就变成了对数似然损失函数：

$$
Loss(x,D) ==\left\{\begin{array}{ll}
 -\log D(x) &\textrm{if $x$ is positive sample}\\
 -\log (1-D(x))&\textrm{if $x$ is negative sample}
\end{array}\right.
$$

最大化似然等效于最小化对数似然函数，最小化对数似然等效于最小化交叉熵。

<br>
假设判别器一次输入$2m$个样本，其中前一半为正样本，后一半为负样本。那么，

$$
正样本的总损失 = -\sum_{i=1}^m \log D(x^{(i)})
$$

$$
负样本的总交损失 = -\sum_{i=m+1}^{2m} (1-\log D(G(z^{(i)})))
$$

$$
\begin{equation}
\begin{split}
2m个样本的平均损失 &= \frac{正样本的总损失+负样本的总损失}{2m} \\
                  &= -\frac{1}{2}(\frac{\sum_{i=1}^m \log D(x^{(i)})}{m}) -\frac{1}{2}(\frac{\sum_{i=m+1}^{2m} (1-\log D(G(z^{(i)})))}{m})
\end{split} 
\end{equation}
$$

$\mathbb{E}{x \sim p_{data}} \log D(x) = \mathbb{E}{x \sim p_{data}} [\log D(x)]$表示取自$p_{data}$的样本也就是正样本的 $\log D(x)$的平均数，也就是$2m个样本的平均损失$左半部分大括号里的内容。同理 $\mathbb{E}_z\log(1-D(G(z)))$表示$2m个样本的平均损失$右半部分大括号里的内容。

所以判别器的损失表示为：

$$
J^{(D)}(\theta^{(D)},\theta^{(G)}) = -\frac{1}{2}\mathbb{E}{x \sim p_{data}} \log D(x) - \frac{1}{2}\mathbb{E}_z(1-D(G(z)))
$$

#### 生成器的损失函数
如果生成器强大，那么判别器误判的概率就越大，即生成器和判别器的损失应该是此消彼长的关系，这个关系可以用简单的零和游戏来描述：
$$
J^{(G)} = - J^{(D)}
$$

#### 零和游戏和minimax
零和游戏是博弈论的一个概念，表示所有博弈方的利益之和为零或一个常数，即一方有所得，其他方必有所失。在零和游戏里，minimax的解跟纳什均衡相同。

minimax理论：对二人零和游戏，存在一个值$V$和一个策略组合，使得

- 给定玩家2的策略，玩家1最大可能的回报(payoff)是$V$
- 给定玩家1的策略，玩家2最大可能的回报是$-V$

在GAN中，判别器是玩家1，生成器是玩家2。$V=V(\theta^{(D)},\theta^{(G)})=-J^{(D)}(\theta^{(D)},\theta^{(G)})$。
minimax的解：
$$
\begin{equation}
\begin{split}
\theta^{(G)*} &= \arg \min_{\theta^{(G)}} \max_{\theta^{(D)}} V(\theta^{(D)},\theta^{(G)}) \\
              &= \arg \min_{\theta^{(G)}} \big(\max_{\theta^{(D)}} V(\theta^{(D)},\theta^{(G)})\big)
\end{split} 
\end{equation}
$$

即先调整$\theta^{(D)}$使得$V(\theta^{(D)},\theta^{(G)})$在$\theta^{(G)}$不变时最大，再调整$\theta^{(G)}$使$V(\theta^{(D)},\theta^{(G)})$最小。

#### 训练算法
判别器和生成器的损失定义好了，如何达到纳什均衡的理论基础也具备了，算法呼之欲出。

<img src="https://nlppupil.github.io/images/gan_alg.png" alt="BP" style="width:600px;height:380px;">


关于算法的几个问题，都在 [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661) 里直接回答了：

- 什么时候判别器达到最优？
- 什么时候生成器达到最优？
- 算法为什么会收敛？什么时候收敛？
- 为什么最小化$V$相当于最小化 $D_{JS}(p_{data}\mid \mid p_{model})$?

<br>
<br>
<br>

*参考资料*
<br>
[Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)
<br>
[NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
<br>
[From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
[Understanding Generative Adversarial Networks](https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/)
<br>
[GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f)
<br>
[Strategies of Play](https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/Minimax.html) 

