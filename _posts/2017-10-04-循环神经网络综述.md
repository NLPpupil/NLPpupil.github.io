---
layout: post
title:  "循环神经网络综述"
date:   2017-10-04
categories: 深度学习
---


<h3>1.数学表示</h3>
RNN的输入是序列，目标(target)也是序列。输入序列用 $x_{1},x_{2},...,x_{T}$ 表示，其中每个 $x_{t}$ 是一个实值向量。目标序列用 $y_{1},y_{2},...,y_{T}$ 表示。训练集一般是（输入，目标）对。

循环神经网络是一种神经网络，也就是函数。普通神经网络可以抽象成输入层，隐层，输出层。隐层不管内部有几层，总体都可以抽象成「一个」隐层。隐层是一个函数，以输入层的数据作为输入，通过计算得到输出。输出层是一个函数，以隐层的输出作为输入，通过计算得到最终该神经网络的输出。一个神经网络是一个函数，该函数是隐层和输出层的复合函数。
RNN的特殊之处在于，它隐层的输入除了输出层，还有上一个时间点（上一次更新）的隐层的输出。初始状态 $h_{0}$，RNN通过 $h_{t} =f(x_{t},h_{t-1})$ 更新。每次更新的输出是 $$\hat{y}_{t}=g(h_{t})$$。

其中，$f$是一个非线性激活函数，不同的$f$决定不同的RNN种类。

<h3>2.Backpropagation Through Time (BPTT)</h3>
BPTT是用于训练RNN的反向传播。如果把经过若干次更新的RNN展开，它的形状就跟普通的多层神经网络一样。BPTT就是一种特殊的反向传播，特殊之处在于每层的权重都是一样的。再具体一点，由于共享权重，在每一个时间点进行反向传播的时候，都要递归地从前一个时间点反向传播，然后求和。推导过程见[Recurrent Neural Networks Tutorial Part 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)。
<h3>3.梯度爆炸和梯度消失</h3>
梯度爆炸和梯度消失指的是梯度的绝对值在训练过程中可能变得极大或极小。举个最简单的例子直观地理解，假设输入向量的维度为0（变成标量）， $f(x^{(t)},h^{(t-1})=wx^{(t)}+h^{(t-1)}$ ，$h^{(0)}=0$。如果权重$w$大于1，随着RNN更新次数增加，最开始的几个输入对当前更新的影响会变得指数级增长，相应的梯度就会爆炸。同理，如果$w$小于1，随着RNN更新次数增加，会出现梯度消失。

<h3>4.Truncated Backpropagation Through Time (TBPTT)</h3>
顾名思义，TBPTT是阉割版的BPTT，它以降低学习长距离依赖性的能力为代价来克服梯度爆炸。具体说来，TBPTT设置了一个最大反向传播距离。


<h3>5.RNN种类</h3>
<h4>5.1 Long Short-Term Memory (LSTM)</h4>
LSTM由Sepp Hochreiter等人提出，克服了梯度消失问题，它的$f$如下操作：

$$
\begin{equation}
 输入节点：g_{t} = \phi(W^{gx}x_{t}+W^{gh}h_{t-1}+b_{g}) \\
 输入门：i_{t} = \sigma(W^{ix}x_{t}+W^{ih}h_{t-1}+b_{i}) \\
 遗忘门：f_{t} = \sigma(W^{fx}x_{t}+W^{fh}h_{t-1}+b_{f}) \\
 输出门：o_{t} = \sigma(W^{ox}x_{t}+W^{oh}h_{t-1}+b_{o}) \\
 内置状态：s_{t} = g_{t}\odot i_{t}+s_{t-1}\odot f_{t}   \\
 h_{t} = \phi(s_{t}) \odot o_{t}
\end{equation}
$$

其中， $\phi$是tanh函数，$\sigma$是sigmoid函数，$\odot$是逐点(pointwise)计算。

直观理解，遗忘门决定了我们多大程度保留上一个内置状态，输入门决定了我们多大程度保留新来的信息，输出门决定了内置状态各维度的比重。这样，LSTM在更新过程中有选择地遗忘旧信息，保留新信息，不断地卸下累赘，以此来克服梯度消失。

<h4>5.2 Gated Recurrent Unit(GRU)</h4>
由Kyunghyun Cho等人提出，它的$f$如下操作：

$$
\begin{equation}
 重置门：r_{t} = \sigma(W^{rx}x_{t}+W^{rh}h_{t-1}+b_{r}) \\
 更新门：z_{t} = \sigma(W^{zx}x_{t}+W^{zh}h_{t-1}+b_{z}) \\
 \widetilde{h}_{t}=Wx_{t}+U(r_{t}\odot h_{t-1})\\
 h_{t} = z_{t} \odot h_{t-1} + (1-z_{t}) \odot \widetilde{h}_{t}
\end{equation}
$$

直观理解，更新门决定隐状态是否用新的 $\widetilde{h}_{t}$ 代替。 当更新门接近0的时候，上一个隐状态就被遗忘了，而新的输入得到保留。这样就有效地遗忘跟当前时刻无关的旧信息。
<h4>5.3 Quasi-Recurrent Neural Networks(QRNN)</h4>
QRNN由James Bradbury等人提出，大大提高了训练效率。详见原文[QUASI-RECURRENT NEURAL NETWORKS](https://arxiv.org/abs/1611.01576)。
<h4>5.4 Simple Recurrent Unit(SRU)</h4>
SRU由Tao Lei等人提出，目的也是提高RNN的训练效率。详见原文[Training RNNs as Fast as CNNs](https://arxiv.org/pdf/1709.02755.pdf)







<br>

<h4>参考文献</h4>

Zachary C. Lipton,*A Critical Review of Recurrent Neural Networks for Sequence Learning*

Kyunghyun Cho, *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*

Sepp Hochreiter and Ju ̈rgen Schmidhuber,*Long short-term memory.*

colah,[*Understanding LSTM Networks*
](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
