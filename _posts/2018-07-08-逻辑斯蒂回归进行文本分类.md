---
layout: post
title:  逻辑斯蒂回归进行文本分类
date:   2018-07-08
categories: 自然语言处理 机器学习
---

本文介绍了简单的逻辑斯蒂回归文本分类。传统机器学习解决文本分类已经比较成熟，也很简单。逻辑斯底回归可以简单理解为将一个特征向量乘以一个矩阵，映射到另一个$C$维向量，矩阵的每一行是对应每一类的权重，矩阵的列数等于特征向量的维度。$C$维向量的每一维对应了$C$个类每一类的几率($logit$)，几率是一个样本是某类的概率除以不是此类的概率。再将几率向量经过一个$softmax$函数，得到了每一类的概率。取概率最大的类作为最终预测结果。本文重点不在逻辑斯蒂回归，而在于获取文本特征向量的数据预处理，和sklearn的基本机器学习流程。

本文特别感谢[基于sklearn的文本分类—逻辑回归](https://blog.csdn.net/laobai1015/article/details/80156506)，在此文基础上进行了一定修改，补充。


#### 数据预处理
数据采用[THUCTC: 一个高效的中文文本分类工具包](http://thuctc.thunlp.org/#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5)中的`THUCNews.zip`。下载，解压，文件结构如下：

<img src="https://nlppupil.github.io/images/thunews.png" alt="thunews" style="width:150px;height:300px;">

每一个文件夹包含了此类的txt文本，每个文本是一篇短新闻。

sklearn最终需要的是一个序列的特征**X**，和一个序列的标签**Y**。对THUNEWS的处理步骤是：

1. 每个新闻类别取$n$条新闻，每个txt文本读到一个`string`并分词，每个类别也是`string`，将新闻**x**和类别**y**放到一个对**(x,y)**。
2. 将所有类别的**(x,y)**放到一起组成**(X,Y)**，shuffle。
3. 将**(X,Y)**拆开为**X**，**Y**。

代码如下：

```python
import glob 
import json
import jieba
from  sklearn.utils import shuffle

articles_per_category = 1000

################ Read data as a (x,y) list ################
data = []
for subdir in glob.glob('../data/THUCNews/*'):
    category = subdir.split('/')[3]
    articles = []
    for filename in glob.glob(subdir+'/*')[:articles_per_category]:
        with open(filename) as f: #打开一篇新闻
            text = ' '.join(jieba.cut(f.read())) #text是新闻内容的分词形式。
            data.append((text,category)) 
            #data是一个序列的（x,y）对


################ Shuffle the data ################
data = shuffle(data)


################ Unzip data into a list of features X and a list of labels Y ################
X,Y = zip(*data)


################ Save the preprocessed data ################
with open('THUCNews1000.X.json', 'w') as f:
    json.dump(X, f,ensure_ascii=False,indent=4)

with open('THUCNews1000.Y.json', 'w') as f:
    json.dump(Y, f,ensure_ascii=False,indent=4)

```

预处理将新闻文本序列存到`THUCNews1000.X.json`，将对应的类别存到`THUCNews1000.Y.json`。

接下来是用sklean获得篇新闻文本的特征向量，然后训练，预测。特征向量的每一维对应一个特征，这里所有文本的词都是特征，即特征向量长度等于词汇表大小。每个特征的值是对应词的if-idf值。
$\mbox{tf-idf} = \mbox{tf} \times \mbox{idf} $。tf表示词频(term frequency)，是一个词在当前新闻文本中出现的次数。idf(inverse document-frequency)衡量了一个词是否在所有文本中都常见，如果一个词在所有文本中都常见，例如“的”，那么显然它对于区分类别帮助较小。idf的计算方式是：

$$
idf(w) = \log \frac{n_{doc}}{df_{(doc,w)}}
$$

其中$n_{doc}$表示总文本数量，$df_{(doc,w)}$表示含有词$w$的文本数量。

要计算每个词的tf-idf值，要分别计算tf和idf。

首先计算tf：

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn import metrics
import json 

with open('THUCNews1000.X.json', 'r') as fp:
    corpus = json.load(fp)

with open('THUCNews1000.Y.json', 'r') as fp:
    category = json.load(fp)

count_vectorizer = CountVectorizer()

X = count_vectorizer.fit_transform(corpus)
```
`CountVectorizer `类用于计算词频。**X**是一个矩阵，每一行是一个文本的稀疏向量，每一列是词表中的一个词，值是当前文本中这个词出现的个数。稀疏的意思是大部分值都为0，因为每行的维度是词汇表大小，但只有极小部分出现了。

然后用词频结果计算idf：

```python
transformer = TfidfTransformer(smooth_idf=False)
X_tfidf = transformer.fit_transform(X)
#tfidf是X经过tf-idf重新分配权重后的矩阵，并且经过L2 normalization。经过L2 normalization的每一行的和并不是1。
#transformer.idf_ 是词表中每个词的idf。
#如过有新的文档，new = vectorizer.transform(['Something completely new.']).toarray()。再将new与transformer.idf_ 做一个点积，
#new_tfidf = new*transformer.idf_。再将new_tfidf做一个L2 normalization，new_tfidf = new_tfidf/numpy.linalg.norm(new_tfidf),
#得到最终的tfidf特征向量。
```
`X_tfidf `的每一行就是一篇文本的最终特征向量。

然后就可以分成训练集和测试集，训练，预测：

```python
#分成训练集和测试集
train_X,test_X = train_test_split(X_tfidf,train_size=0.9,shuffle=False)
train_Y,test_Y = train_test_split(category,train_size=0.9,shuffle=False)

clf = linear_model.LogisticRegression(n_jobs=-1).fit(train_X, train_Y)
predicted = clf.predict(test_X)
print(metrics.classification_report(test_Y, predicted))
print('accuracy_score: %0.5f' %(metrics.accuracy_score(test_Y, predicted)))
```

结果如下：

<img src="https://nlppupil.github.io/images/lrclass.png" alt="result" style="width:420px;height:300px;">



<br>
<br>
<br>
<br>
<br>

*参考资料*

[sklearn Feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)

