---
layout: post
title:  "反向传播"
date:   2017-10-21
categories: 深度学习
---

如何直观地、形象地理解反向传播？经常有人这么问。言下之意是，不看数学公式如何理解反向传播？我的答案是，即使可以“形象地”理解反向传播，也是似懂非懂，根基不牢。数学推导才是正道，只要会求偏导数，理解反向传播其实不难。

<h4>复合函数角度</h4>
首先从复合函数的角度推导。材料来自南京大学吴建鑫教授的[Introduction to Convolutional Neural Networks](https://cs.nju.edu.cn/wujx/paper/CNN.pdf)。任何深度神经网络都可以抽象成一系列复合函数。记第一层（最里面的函数）的输入是$x^{1}$，输出是$x^{2}$（也是第二层的输入），参数是$w^{1}$。以此类推，最后一层（最外面的函数）的输入是$x^{L}$，输出是$z$，参数是$w^{L}$。最后一层是计算损失的层，$z$是损失。

参数通过随机梯度下降更新：

$$
\begin{equation}
w^{i} \longleftarrow w^{i}-\mu \frac{\partial z}{\partial w^{i}}
\end{equation}
$$

也就是说，对每个参数的更新，都要知道损失对该参数的偏导数。反向传播就是干这个的。

假设我们想求 $\frac{\partial z}{\partial w^{i}}$，我们要把它跟第$i+1$层联系起来：

$$
\begin{equation}
\frac{\partial z}{\partial w^{i}}=\frac{\partial z}{\partial x^{i+1}}\cdot \frac{\partial x^{i+1}}{\partial w^{i}}
\end{equation}
$$

这是一个简单的链式法则。其中 $\frac{\partial x^{i+1}}{\partial w^{i}}$ 是第$i$层的输出对该层参数的偏导，如果知道 $x^{i+1}$ 就可以直接计算，这就要求我们存储每一层的输出。然后我们要计算 $\frac{\partial z}{\partial x^{i+1}}$ ，如果 $x^{i+1}$ 是最后一层的输入，那么 $\frac{\partial z}{\partial x^{i+1}}$ 可以直接计算。如果 $x^{i+1}$ 不是最后一层的输入怎么办呢？这就要求我们存储损失对每一层输入的偏导。所以我们还需要另一个式子：

$$
\begin{equation}
\frac{\partial z}{\partial x^{i}}=\frac{\partial z}{\partial x^{i+1}}\cdot \frac{\partial x^{i+1}}{\partial x^{i}}
\end{equation}
$$

这里 $\frac{\partial z}{\partial x^{i+1}}$ 是之前计算好的已经存储起来，$\frac{\partial x^{i+1}}{\partial x^{i}}$ 可以直接计算。

梳理一遍：从最后一层开始，计算损失对该层参数的偏导和对该层输入的偏导并存储起来。对于每层的两个偏导计算，都需要上一层的计算结果。这个过程一直进行到第一层，它是”反向”的，偏导一直向第一层“传播”。

<h4>神经网络角度</h4>
复合函数角度是宏观，神经网络角度是微观。复合函数角度是道，神经网络角度是术。材料来自南京大学周志华教授的《机器学习》。假设我们训练多层前馈神经网络，有$d$个输入神经元，$l$个输出神经元，$q$个隐层神经元。隐层和输出层都用Sigmoid作为激活函数。训练集
$D = \{ (x^1,y^1),(x^2,y^2),...,(x^m,y^m)\}$。如下图所示：

<img src="https://nlppupil.github.io/images/BP.jpeg" alt="BP" style="width:400px;height:250px;">

![BP](https://nlppupil.github.io/images/BP.jpeg)
