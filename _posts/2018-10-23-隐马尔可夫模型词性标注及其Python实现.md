---
layout: post
title:  "隐马尔可夫模型词性标注及其Python实现"
date:   2018-10-23
categories: 自然语言处理 机器学习
---

本文介绍HMM词性标注及其Python实现，包括HMM的三个基本问题：概率计算问题、学习问题和预测问题。实现只用了Python内置模块。

### 1.模型简介
按照李航《统计学习方法》中的定义，HMM由三要素组成：初始状态概率向量$\pi$，状态转移概率矩阵$A$和观测概率矩阵$B$（发射矩阵）组成。可表示为，

$$
\lambda = (A,B,\pi)
$$

HMM有三个基本问题：

1. 概率计算问题。给定HMM，计算某个观测序列的概率。
2. 学习问题。学习HMM的参数。
3. 预测问题，也成为解码问题。给定观测序列，求最有可能的状态序列。

### 2.HMM词性标注
词性标注是标注问题，标注问题指的是输入变量与输出变量均为变量序列的预测问题。在HMM词性标注中，状态是词性，观测是词。一个句子的每个词都对应了一个词性，词性标注就是把一个句子的每个词都标注上词性。

HMM中，状态之间的转移由转移概率决定，相应的观测序列由发射概率决定，一个时刻的观测跟其他时刻的观测无关。在HMM词性标注中，转移概率$transition[i][j]$是词性$i$条件下下一个词性是$j$的概率，即$P(j \mid i)$；转移概率$emission[i][w]$是词性$i$产生词$w$的概率，即$P(w \mid i)$。

在HMM词性标注中，词性标注是解码问题。

### 3.数据
学习模型需要数据。词性标注训练数据的样例是一个句子和相应的词性，例如：
                     
>处在/v 抗战/n 已经/d 过/vd 了/u 三十一/m 个/q 月/nt 的/u 今日/nt ，/w 处在/v 客观/a 的/u 形势/n 更加/d 需要/v 我们/r 去/v 动员/v 全国/n 人力/n 物力/n 财力/n 知/v 力/n 以/c 支持/v 抗战/n 的/u 今日/nt ，/w 这个/r 节日/nt 之/u 来临/v ，/w 是/vl 具有/v  重大/a 的/u 意义/n 的/u ，/w 因为/c 抗战/n 建国/v 的/u 大业/n ，/w 如果/c 没有/d 占/v 人口/n 半数/n 的/u 妇女/n 来/vd 参加/v ，/w 成功/v 显然/a 是/vl 不/d 可能/vu 。/w  

本实现所用的数据来自[国家语委现代汉语语料库](http://www.cssn.cn/yyx/yyxcyzy/201509/t20150922_2423182.shtml)中的12万句。下载点击[现代汉语语料库检索](http://corpus.zhonghuayuwen.org/CnCindex.aspx)，在搜索框输入关键词，选择标注语料，然后点击检索->下载语料。
<img src="https://nlppupil.github.io/images/语料库检索.png" alt="BP" style="width:800px;height:200px;">

生数据预处理成一个列表的句子，一个句子是一个列表的(词,词性)对，存储在json文件中。训练数据有110000句，3310354词；测试数据有9855句，296317词。

词性代码的意义见下表：

代码 | 意义 | 代码 | 意义 | 代码 | 意义 | 代码 | 意义 | 代码 | 意义 |
---- | --- | ---- | --- |---- | --- | ---- | --- |---- | --- |
a | 形容词 | aq|性质形容词|as|状态形容词|c|连词|d|副词
e|叹词|f|区别词|g|语素字|ga|形容词性语素字|gn|名词性语素字
gv|动词性语素字|h|前接成分|i|习用语|ia|形容词性习用语|ic|连词性习用语
in|名词性习用语|iv|动词性习用语|j|缩略语|ja|形容词性缩略语|jn|名词性缩略语
jv|动词性缩略语|k|后接成分|m|数词|n|名词|nd|方位名词
ng|普通名词|nh|人名|ni|机构名|nl|处所名词|nn|族名
ns|地名|nt|时间名词|nz|其他专有名词|o|拟声词|p|介词
q|量词|r|代词|u|助词|v|动词|vd|趋向动词|
vi|不及物动词|vl|联系动词|vt|及物动词|vu|能愿动词|w|其他
wp|标点符号|ws|非汉字字符串|wu|其他未知符号|x|非语素词

### 4.实现
#### 数据结构
如何表达HMM三要素？定义里包括的一个向量两个矩阵，都可以用Python字典类型表达。比如$\pi[i]$表示第$i$个状态的初始概率，那么在代码中可以把$i$视为某个状态实例（而不是一个数字）：

```
pi = {state:prob}
transition = {state:{state:prob}} #transition[q1]是一个字典，存储q1后面跟随的每个状态的概率。
#transition[q1][q2] = P(q2 | q1)
emission = {state:{observation:prob}} #类似transition
```

#### 学习问题：极大似然估计
概率计算问题和解码问题都要依赖HMM参数，所以第一步要学习HMM的参数。类似朴素贝叶斯分类器的训练，采用极大似然估计，通过计数来学习HMM参数。

初始状态概率的学习：$\pi[q] = \frac{词性q出现在所有训练句子开头的次数}{总训练句子数}$

转移概率的学习：$transition[q_1][q_2] = \frac{所有训练句子的所有相邻词性二元组中，q_1词性后面跟着q2的次数}{所有训练句子的所有相邻词性二元组中，第一个词性是q_1的二元组个数}$

发射概率的学习：$emission[q][w] = \frac{所有句子中所有的词性-词二元组中，词性q发射词w的次数}{所有句子中所有的词性-词二元组中，词性是q的个数}$

完整代码见项目目录里的`learn_hmm.py`，核心代码如下：

```
for sent in train_sents:
    #学习初始概率，sent[0][1]是句子sent第一个(词，词性)对的词性，也就是本句第一个词性
    pi_freq[sent[0][1]] += 1

    #学习转移概率
    #states_transition是一个列表的状态对，记录了词性的转移过程，例如
    states_transition = [(p1[1],p2[1]) for p1,p2 in zip(sent,sent[1:])]
    for p1,p2 in states_transition:
        transition_freq[p1][p2] += 1
    
    #学习发射概率
    for w,p in sent:
        emission_freq[p][w] += 1
```

#### 解码问题：维特比算法
训练完HMM后，就可以开始词性标注了，这是解码问题。

解码问题用的是维特比算法，详见[维特比算法](https://nlppupil.github.io/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/10/20/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95.html)。

完整代码见`viterbi.py`，核心代码如下：

```python
def argmax(t,s):
        '''
        计算对于t时刻s状态的维特比变量，顺便记录它是由哪一个状态转移而来
        '''
        max_prob, argmax_pre_state = 0,0
        for i in states:
            try:
                p = viterbi_matrix[t-1][i]*transition[i][s]*emission[s][obs[t]]
            except:
                p = 0
            if p > max_prob:
                max_prob = p
                argmax_pre_state = i 

        return max_prob,argmax_pre_state
    
    #递推
    for t in range(1,T):
        for s in states: 
            max_prob,argmax_pre_state = argmax(t,s)
            viterbi_row.append(max_prob)
            viterbi_matrix[t][s] = max_prob
            backpointers_matrix[t][s] = argmax_pre_state
```

#### 概率计算问题：前向算法
本节需要先看完维特比算法。
<br>
首先温习一下全概率定理：
<br>
设$A_1,A_2,...,A_n$是一组互不相容的事件，形成样本空间的一个分隔（每一个试验结果必定使得其中一个事件发生）。又假定对于每一个$i$，$P(A_i) > 0$。则对于任何事件$B$，下列公式成立

$$
\begin{equation}
\begin{split}
P(B) &= P(A_1 \cap B) + ... + P(A_n \cap B)\\
     &= P(A_1)P(B \mid A_1) + ... + P(A_n)P(B|A_n)
\end{split} 
\end{equation}
$$

根据全概率定理，一个长度为$T$的观测序列的概率等于它跟所有可能的状态序列的联合概率的和。如果直接计算，一共有$N^T$个可能的状态序列，这种方法不可行。

记$P(t)$为$t$时刻时观测序列的概率，如果知道$P(t)$，如何计算$P(t+1)$？$t+1$时刻的状态$q_{t+1}$可能转移自所有状态，这就要求将$P(t)$的事件分成$N$个分割。每个分割都可以生成$q_{t+1}$，把这$N$个分割分别对新加入的$q_{t+1}$和观测$o_{t+1}$求联合概率，就得到$N$个分割分别的概率。而$q_{t+1}$又有$N$中可能，构成$P(t+1)$对应事件的一个分割。所以关键在于求$t$时刻每个分割的概率，这就是前向算法。

定义到时刻$t$部分观测序列为$o_1,o_2,...,o_t$且状态为$q_i$的概率为前向概率，记作

$$
\alpha_t(i) = P(o_1,o_2,...,o_t,i_t = q_i \mid \lambda)
$$

可以递推地求得前向概率$\alpha_t(i)$及观测序列概率$P(O \mid \lambda)$。

**观测序列概率的前向算法**
<br>
输入：隐马尔可夫模型$\lambda$，观测序列$O$
<br>
输出：观测序列概率$P(O \mid \lambda)$
<br>
(1)初值
<br>
初值等于词性初始概率乘以发射概率
$$
\alpha_t(i) = \pi[i]emission[i][o_1], i \in Q
$$

(2)递推
<br>
对$t=1,2,...,T-1$,
<br>
$t$时刻，对于每个结尾状态，它可能转移自$N$个状态，应用全概率定理

$$
\alpha_{t+1}(i) = \sum_{j \in Q}\alpha_t^{j}\times transition[j][i]\times emissionn[i][o_{i+1}], i \in Q
$$

(3)终止
<br>
应用全概率定理，

$$
P(O \mid \lambda) = \sum_{i \in Q} \alpha_T(i)
$$

每次递推需要$N \times N$次计算，递推$T$次，复杂度是$O(TN^2)$。
完整代码见`forward.py`，核心代码如下：

```python
#递推
    for t in range(1,T):
        previous_alphas = alphas
        alphas = []
        for s in states:
            braket = np.dot(previous_alphas,[dis[s] for dis in transition.values()])
            alphas.append(braket*emission[s][observations[t]])
```

#### 评估
每个词必须且只能标记一个词性，所以精确度=查准率=查全率。
评估完整代码见`measure.py`，核心代码如下：

```python
for t in test_data:
        sent, tags = zip(*t)
        preds = viterbi(transition,emission,pi,sent) 
        total_tags.extend(tags)
        total_preds.extend(preds)

    correct_num = sum([t==p for t,p in zip(total_tags,total_preds)])

    print ('准确率{0:.2f}%'.format(correct_num*100/len(total_preds)))
```
在此训练集和测试集下，准确率是92.73%。
