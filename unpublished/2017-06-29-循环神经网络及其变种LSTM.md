---
layout: post
title:  "循环神经网络及其变种LSTM"
date:   2017-06-29 17:00:32
categories: 深度学习
---

首先说一句，对于理解RNN这个概念，上策是先从数学角度理解，然后再画图理解。在这个前提下，一切上来就放图的书、论文、教程都是耍流氓。
循环神经网络是一种神经网络，那数学上神经网络是什么？是函数。普通神经网络可以抽象成输入层，隐层，输出层。隐层不管内部有几层，总体都可以抽象成“一个”隐层。隐层是一个函数，以输入层的数据作为输入，通过计算得到输出。输出层是一个函数，以隐层的输出作为输入，通过计算得到最终该神经网络的输出。一个神经网络是一个函数，该函数是隐层和输出层的复合函数。
RNN的特殊之处在于，它隐层的输入除了输出层，还有上一个时间点（上一次更新）的隐层的输出。严格地，在每个时间的点$$t$$，隐层$$h_{t}$$都通过下式更新：

$$
\begin{equation}
  h_{t} = f(h_{t-1}, x_{t})
\end{equation}
$$

其中，$$f$$是一个非线性激活函数，简单的$$f$$可以是sigmoid函数，复杂的$$f$$可以是一个long short-term memory(LSTM)单元。$$f$$其实还有参数$$W$$和$$b$$，式子里没表示出来，我们训练的就是参数。RNN的输出层就是普通的输出层。也就是，在每一个时间点，RNN输入一次，然后输出一次。一系列时间点后，总的看来RNN就好像输入一个序列，然后输出一个序列。这个隐层可以由一个函数构成（一个节点），也可以由许多函数复合而成（n个节点构成的网）。不管怎样，组成隐层的每一个函数的输入，都是上一个时间点自身的输出和当前时间点的输入层。
接下来进入画图环节。我们在别的资料上经常看到的那些奇形怪状的弯弯曲曲的歪歪扭扭的图是怎么回事呢？其实如果画成立体图就显而易见了。我们把RNN想象成一张网，随着时间点的增加，这张网沿着它的法线进行平移，其中隐层之间有箭头连接，由上一个隐层指向下一个隐层，图就是这么画出来的。我们运用的时候，RNN就是一张网，而不是一摞网。一摞网是我们模拟计算过程时想象出来的。
到这里RNN的本质就说完了，细节我就不说了。接下来轮到LSTM。当我们说LSTM的时候，一般指的是运用LSTM思想的循环神经网络，而为了有助理解，不产生混淆，在这里我说的LSTM指的是之前提到的LSTM 单元。
LSTM单元虽然名字叫单元，但它就是个函数。这个函数以上一个隐层和当前输入层作为输入，它的输出就是当前隐层的输出。LSTM单元是这样运算的：

$$
\begin{equation}
 输入节点：g_{t} = \phi(W^{gx}x_{t}+W^{gh}h_{t-1}+b_{g}) \\
 输入门：i_{t} = \sigma(W^{ix}x_{t}+W^{ih}h_{t-1}+b_{i}) \\
 遗忘门：f_{t} = \sigma(W^{fx}x_{t}+W^{fh}h_{t-1}+b_{f}) \\
 输出门：o_{t} = \sigma(W^{ox}x_{t}+W^{oh}h_{t-1}+b_{o}) \\
 内置状态：s_{t} = g_{t}\odot i_{t}+s_{t-1}\odot f_{t}   \\
 h_{t} = \phi(s_{t}) \odot o_{t}
\end{equation}
$$

不要被这些名字所迷惑，反正就是通过一系列辅助函数进行运算，最后的$h_{t}$就是当前隐藏层的输出。
到这里LSTM的数学表示也说完了，很简洁，还看什么图呢？
至于RNN的训练问题：梯度消失和梯度爆炸，以及LSTM从哪方面对RNN进行改进，就要自己看了。多看著名论文，少看博客。





<br>
<br>


参考资料:
《A Critical Review of Recurrent Neural Networks for Sequential Learning》
《Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation》˛
