---
layout: post
title:  "论文阅读：《A Neural Probabilistic Language Model》"
date:   2017-09-30
categories: 论文阅读 深度学习 自然语言处理
---

**这是2003年的一篇论文，作者是Bengio等人。这篇论文发明了神经语言模型，该模型同时学习了（1）每个词的分布式表示。（2）词序列的概率函数。**


这个模型的主要思想有：
1. 为词汇表中的每一个词分配一个分布式的词特征向量（$m$维实值向量），
2. 词序列的联合概率函数，
3. 同时学习词特征向量和概率函数的参数。

接下来严格表述一下：

训练集是一序列词$w_{1},..,w_{T}$，词汇表是$V$。目标是学习一个模型$f(w_{t},...,w_{t-n+1}) = \hat{P}(w_{t}\mid w_{1}^{t-1})$（连续n个任意词出现的概率分布）。
把函数$f$分解成两部分：
1. 一个映射$C$，从词汇表中任意元素映射到一个$m$维实值向量。实践中，$C$用一个 $\| V\| \times m$ 的矩阵表示。
2. 一个映射$g$把上下文(前$n-1$个词)的一序列词特征向量 $(C(w_{t-n+1},...,C(w_{t-1})))$ 映射到词汇表的概率分布（每一个词都有一个概率，输出的概率分布的第$i$个元素就是词汇表第$i$个词的在上下文下的条件概率。

函数$f$是$C$和$g$的复合函数，$C$被所有词共享。这两个子函数都有参数，$C$的参数就是 $\|V\| \times m$ 矩阵，第$i$行是第$i$个词的表示。$g$可以用前馈神经网络或循环神经网络实现，参数是 $\omega$。总体参数就是 $\theta = (C,\omega)$。训练过程是找到使训练语料库的带惩罚的对数似然最大的 $\theta$：

$\begin{equation}
L = \frac 1 T \sum_{t} \log f(w_{t},w_{t-1},...,w_{t-n+1};\theta) + R(\theta)
\end{equation}$

这里 $R(\theta)$ 是正则项。
