---
layout: post
title:  "论文阅读：《Neural Machine Translation and Sequence-to-sequence Models: A Tutorial》"
date:   2017-06-30
categories: 论文阅读 深度学习 自然语言处理
---

这是CMU的助理教授Graham Neubig的一篇tutorial的翻译，原文来自[Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619)。本文只取其中重要部分，略掉习题和高级领域。
>#### 2 统计机器翻译
>任何翻译系统都可以定义成一个函数，输入是原句，输出是一个翻译假设。统计机器翻译系统是用给定原句下译句的概率分布来进行翻译的系统。找到最大的译句，使得条件概率最大。模型的参数由平行语料库习得。
为了创建一个好的翻译系统，有三个主要问题需要解决：
>
> - 建模：模型长什么样子，有什么参数，参数如何确立概率分布。
> - 学习：从训练数据学习参数取值。
> - 搜索：最后我们需要寻找最有可能的译句。这个过程通常叫解码。
>
>#### 3 $$n$$元语言模型
>语言模型是句子的概率分布，机器翻译中需要目标语言的语言模型。语音模型有两个用处：
>
>- 评估自然程度：给定一个句子，语言模型可以分辨这个句子多大程度上是这个语言的自然句。
>- 产生文本：根据语言模型的概率分布随机生成文本，这可以告诉我们语言模型“认为”什么是自然句。

解读：关于n元语言模型，可以看我的另一篇文章[ N元语言模型](https://wuhaixutab.github.io/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017/06/29/N%E5%85%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html)。
>
>#### 3.1 逐词计算概率
>我们感兴趣的是计算一个句子$$E = e_{1}^{T}$$的概率:
>
>$$
\begin{equation}
P(E) = P(|E| = T,e_{1}^{T})
\end{equation}
>$$
>
>句子长度是$$|E|=T$$，第一个词是$$e_{1}$$，第$$T$$个词是$$e_{T}$$。
>一个句子的概率，可以由单词的条件联合概率求出：
>
>$$
\begin{equation}
P(E) = \prod_{t=1}^{T+1} P(e_{t} \bracevert e_{1}^{t-1})
\end{equation}
>$$
>
>在这里$$e_{T+1} = </s>$$，表示句尾。
>
>语言模型问题就变成了给定一个词计数下一个词的个数的问题。
>
>#### 3.2 基于计数的$$n$$元语言模型
>
>$$
\begin{equation}
P_{ML}(e_{t} \bracevert e_{1}^{t-1}) = \frac{c_{prefix}(e_{1}^{t})}{c_{prefix}(e_{1}^{t-1})}
\end{equation}
>$$
>
>这个式子表示在前$$t-1$$个词的条件下，第$$t$$个词的概率。$$c_{prefix}(.)$$是这个分句出现在句首的次数。这个方法叫做极大似然估计（MLE）。
>
>然而，对于未出现在训练数据的句子，概率模型会得到0。为了解决这个问题，我们设置一个移动窗口，把条件概率链式法则简化成固定长度的链式法则：
>
>$$
\begin{equation}
P(e_{t} \bracevert e_{1}^{t-1}) \approx P_{ML}(e_{t} \bracevert e_{t-n+1}^{t-1})
\end{equation}
>$$
>
>经过这种简化的语言模型就叫做n元语言模型。

解读：n元语言模型在计算条件概率的时候往前看$n-1$个词，比如二元语言模型就往前看一个词。记概率分布为A，数据为B。A对B的似然表示“A有多大可能生成B，B有多大可能是由A生成的”。记两个事件分别为C，D。C在D条件下的概率等于D对于C的似然。

>n元语言模型的参数$$\theta$$由所有条件概率（某个单词给定前$$n-1$$个单词条件下的概率）组成:
>
>$$
\begin{equation}
\theta_{e_{t-n+1}^{t}} = P(e_{t} \bracevert e_{t-n+1}^{t-1})
\end{equation}
>$$
>
>参数可由极大似然估计求得：
>
>$$
\begin{equation}
\theta_{e_{t-n+1}^{t}} = P_{ML}(e_{t} \bracevert e_{t-n+1}^{t-1})= \frac{c(e_{t-n+1}^{t})}{c(e_{t-n+1}^{t-1})}
\end{equation}
>$$
>
>这里$$c(.)$$是语料库中任何地方某个子句的个数。
>如果我们遇到语料库中没有的子句，也会得到概率0。解决办法是平滑。目前，通常被认为标准有效的平滑方法是Modified Kneser-Ney smoothing (MKN)。
>
>#### 3.3 语言模型的评估
>最直接的定义精确度的方法是模型对于测试数据的似然，等于模型赋予数据的概率：
>
>$$
\begin{equation}
P(\varepsilon_{test};\theta)
\end{equation}
>$$
>
>通常假设数据由相互独立的句子或文件E组成，我们有：
>
>$$
\begin{equation}
P(\varepsilon_{test};\theta) = \prod_{E \in \varepsilon_{test}}P(E;\theta)
\end{equation}
>$$
>
>另一个测度是对数似然：
>
>$$
\begin{equation}
log P(\varepsilon_{test};\theta) = \sum_{E \in \varepsilon_{test}}logP(E;\theta)
\end{equation}
>$$
>
>另一个常用测量是困惑度：
>
>$$
>\begin{equation}
ppl(\varepsilon_{test};\theta) = e^{-(log P(\varepsilon_{test};\theta))/length(\varepsilon_{test})}
\end{equation}
>$$
>
>困惑度的一个直观解释是，“模型对于它的决定有多困惑?”。更确切地，“如果我们每一步从概率分布中随机挑选单词，平均需要挑选多少个单词才能得到正确的那个?”
>
>#### 3.4 处理未知词
>测试数据中未出现在训练数据的单词叫做未知词，处理未知词的方法有：
>
>- 假设封闭的词汇表： 假设测试集上不会出现新的单词。
>- Interpolate with an unknown words distribution
>- 添加一个\<unk\>单词： 把训练集中的一些单词用\<unk\>替换。
>
>#### 4 对数线性语言模型
>本节讨论另一种语言模型：对数线性语言模型。它的计算方法跟上面提到的基于计数的语言模型很不同。
>
>#### 4.1 模型公式化
>对数线性语言模型也是计算给定前几个词的条件下下一个词的条件概率，但是方法不一样，基本分为以下几步：
>
>- 计算特征： 对数线性语言模型围绕 ***特征*** 这个概念。特征是“上下文中某个对预测下一个词有用的东西”。更精确地，我们定义一个特征函数$$\phi(e_{t-n+1}^{t-1})$$，以上下文为输入，输出一个实值 ***特征向量*** $$x \in \mathbb{R}^{N}$$来用$$N$$个特征描述上下文。
>- 计算得分： 有了特征向量之后，我们就要用它预测每个单词的概率。为此，我们计算一个得分向量$$s \in \mathbb{R}^{\vert V\vert}$$对应每个词的似然。我们用模型参数$$W \in \mathbb{R}^{\vert V\vert \times N},b \in\mathbb{R}^N$$来计算得分向量：
>
>$$
\begin{equation}
s = Wx + b
\end{equation}
>$$
>
>- 计算概率： 把得分向量转换成概率向量：
>
>$$
\begin{equation}
p = softmax(s)
\end{equation}
>$$

解读：$$p$$表示了在上下文$$e_{t-n+1}^{t-1}$$之后，每个词出现的概率。$$p$$本身是长度为$$|V|$$的向量，向量里的值表示相应位置的词出现的概率。每个词都有一个index，这个index跟$$p$$的index相对应。
>#### 4.2 学习模型参数
>首先我们要定义损失函数$$\mathscr{l}(.)$$，一个表达我们在训练数据上做的有多差的函数。大多数情况下，我们假设这个损失等于负对数似然：
>
>$$
\begin{equation}
\mathscr{l}(\varepsilon_{test};\theta) = -log P(\varepsilon_{test} \bracevert \theta) = - \sum_{E \in \varepsilon_{train}} logP(E \bracevert \theta)
\end{equation}
>$$

解读：这里的参数不再是$$P(e_{t} \bracevert e_{t-n+1}^{t-1})$$，而是$$W,b$$。不同的参数得到不同的损失，我们要通过训练找到那个使损失最小的参数。比如，我们要计算一个句子$$E$$的损失，就计算参数对于$$E$$的负对数似然。通过之前的 **计算概率** 公式依次计算$$E$$的分句的条件概率，再用链式法则计算参数对于$$E$$的似然$$P(E \bracevert \theta)$$，相应也得到了负对数似然。
>用随机梯度下降更新参数。为了要保证训练过程稳定，还有其他东西要考虑：
>
>- 调整学习率： 一开始学习率比较大，然后逐渐减少。
>- 早停： 通常会留出一个开发集(验证集)，在这个集上测量对数似然，然后保留那个最大对似然的模型。这是为了防止过拟合。另一个防止过拟合的办法是当开发集上的对数似然停止提高的时候减小学习率。
>- 洗牌顺序： 有的时候顺序有所偏向，为了防止最后训练的模型更贴合结尾部分的数据，我们需要把整个数据训练的顺序洗牌。
>
>#### 4.3 对数线性模型的导数
>略
>
>#### 4.4 语言模型的其他特征
>对数线性模型好的原因是它允许我们灵活地选择我们认为对预测下一个词有用的特征，包括：
>
>- 上下文特征： 如之前所说。
>- 上下文类： 把相似的词归为一类。
>- 词后缀特征： 比如ing。
>- 词袋特征： 跟只用句子里前n个词相反，我们可以用之前所有的词，然后不顾顺序。这样我们会损失排列信息，但是会知道哪些词会一同出现。
>
>#### 5 神经网络和前馈语言模型
>略
>
>#### 6 循环神经网络语言模型
>6.1 - 6.4 略
>
>#### 6.5 Online, Batch, and Minibatch Training
>对于每一个样例进行参数更新的学习叫做online学习。与之相反，batch学习将整个训练集视为单个单元，计算这个单元的梯度，然后在遍历所有数据之后进行参数更新。
>
>这两个更新策略各有权衡：
>
>- online学习更快地找到一个好的答案。
>- 训练结束后，batch学习更稳定，因为它不受最后看见的数据的影响。
>- batch学习更容易陷入局部最优解。online学习的随机性使得模型能够跳出局部最优，找到一个全局最优。
>
>minibatching是以上两者的折中。minibatching每次计算n个训练样例的梯度。
>
>#### 7 神经编码器-解码器模型
>之前的节我们都关注计算$$P(E)$$的语言模型，现在我们回到机器翻译，对$$P(E|F)$$，给定$$F$$时$$E$$的概率进行建模.
>
>#### 7.1 编码器-解码器模型
>编码器把原句编码到一个向量，解码器把这个向量解码成译句。

解读：详细可以看另一篇论文[论文阅读：《Neural Machine Translation by Jointly Learning to Align and Translate》](https://wuhaixutab.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate.html)。
>#### 7.2 生成输出
>大体上，我们生成输出的时候需要根据不同的标准：
>- 随机抽样： 从概率分布中随机选择输出。
>- 1-最好搜索： 找到使条件概率最大的那个译句。
>- n-最好搜索： 找到前n个最好的译句。
>
>#### 7.2.1 随机抽样
>当我们需要结果不是一成不变的时候，随机抽样就有用了，比如对话系统。每次随机抽样一个词，然后根据之前翻译出来的词，随机抽样下一个词，直到输出完整的译句。这种方法叫做祖传抽样。
>
>#### 7.2.2 贪婪1-最好搜索
>跟上一个方法类似，区别在于每次生成新词的时候，不是随机抽样，而是找到最好的那一个。虽然个词都是当时最好的，但是整句话不一定是条件概率最大的。
>
>#### 7.2.3 光束搜索
>光束搜索解决了贪婪1-最好搜索的问题。与之类似，不同的是每次我们找出$$b$$个最好的结果，$$b$$是光束的宽度。解码器在用光束搜索解码的时候，更偏向短句。因为每增加一个词，就降低了句子整体的概率。当我们增加光束宽度的时候，光束搜索在短句上表现的更好。结果是大的光束宽度对短句有偏向。 ​​
![beam](https://wuhaixutab.github.io/images/beam.png)
>
>#### 7.3其他的编码方式
>#### 7.3.1 倒序和双向编码器
>反向编码器就是就是将原句序列倒序输入编码器，这么做的动机是对于有相似顺序的语言对，比如英语-法语，互译词的顺序是差不多的。如果原句正序输入编码器，互译词的距离将是原句的长度。倒序就解决了这个问题。
>
>双向编码器就是将原句正序和倒序都进行编码，让后将两个编码结合起来。

解读：关于双向编码器可以看另一篇论文[论文阅读：《Neural Machine Translation by Jointly Learning to Align and Translate》](https://wuhaixutab.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate.html)。
>#### 7.3.2 卷积神经网络
>略
>
>#### 7.3.3 树结构网络
>略
>
>#### 7.4 多模型集成
>不同的模型会犯不同的错误，如果我们把多个模型集成起来，错误就会减少。
>
>第一步是分别训练多个模型，然后在解码的时候，概率取这几个模型的平均值。
>
>#### 8 有注意的神经机器翻译
>在之前的模型里，编码器把原句编码到一个固定长度的向量。然而，这么做过度简化了。我们介绍了 ***注意*** 来克服这个问题。
>
>#### 8.1 编码器-解码器中的表示问题
>有两件事困扰着标准的编码器-解码器结构。一个是之前提到的长距离依赖，之前提到用倒序来缓解这个问题。
>
>第二个，就是编码器把任意长度的句子编码到固定长度的向量。如果我们的网络太小，就没能力把信息全部编码到固定长度的向量。另一方面，即使我们有足够大的网络，遇到短句子可能大材小用，浪费内存和计算资源。
>
>本节余下部分讨论我们解决这个问题的办法： ***注意***。
>
>#### 8.2 注意
>注意的基本思想是与其用一个固定长度的向量表示原句，不如我们保留围绕每一个原句词的所有向量，然后在解码的每一步都参考这些向量。
>
>首先，我们用双向RNN对每一个原句词都计算一个向量：
>
>$$
\begin{equation}
\overrightarrow{h}_{j}^{(f)} = RNN(embed(f_{j}),\overrightarrow{h}_{j-1}^{(f)}) \\
\overleftarrow{h}_{j}^{(f)} =RNN(embed(f_{j}),\overrightarrow{h}_{j+1}^{(f)})
\end{equation}
>$$
>
>然后我们可以把这两个向量连接在一起：
>
>$$
\begin{equation}
h_{j}^{(f)} = [\overrightarrow{h}_{j}^{(f)},\overleftarrow{h}_{j}^{(f)}]
\end{equation}
>$$
>
>我们可以进一步把这些向量连接成矩阵：
>
>$$
\begin{equation}
H^{(f)} = concat\_col(h_{1}^{(f)},...,h_{\vert F \vert}^{(f)})
\end{equation}
>$$
>
>这个矩阵每一列对应原句的一个词。
>
>注意的关键洞见是我们计算一个$$\alpha_t$$可以用来把$$H$$组合成一个向量$$c_t$$:
>
>$$
\begin{equation}
c_t = H^{(f)} \alpha_t
\end{equation}
>$$
>
>这个$$\alpha_t$$叫做 ***注意向量***，假设元素的和是1。注意向量告诉我们应该注意原句的哪一个词。
>
>#### 8.3 计算注意得分
>接下来问题就变成了，我们从哪里得到$$\alpha_t$$？答案在解码器 RNN。解码器的隐状态$$h_t^{(e)}$$是一个表示之前的译句词$$e_{1}^{t-1}$$的向量，初始化是$$h_0^{(e)} = h_{\vert F \vert +1}^{f}$$.这被用来计算上下文向量$$c_t$$，初始化$$c_0 = \boldsymbol{0}$$。
>
>首先,我们更新隐状态$$h_t^{(e)}$$:
>
>$$
\begin{equation}
h_t^{(e)} = enc([embed(e_{t-1});c_{t-1}],h_{t-1}^e)
\end{equation}
>$$
>
>基于这个$$h_t^{(e)}$$，我们计算 ***注意得分*** $$a_t$$:
>
>$$
\begin{equation}
a_{t,j} = attn\_score(h_j^{(f)},h_t^{(e)})
\end{equation}
>$$
>
>这个 $$attn\_score(.)$$ 可以是任意的函数，以两个向量作为输入，输出一个得分来表达$$h_t^{(e)}$$有多关注$$h_j^{(f)}$$.
>然后进行归一化：
>
>$$
\begin{equation}
\alpha_t= softmax(a_t)
\end{equation}
>$$
>
>现在我们有了上下文向量$$c_{t}$$和隐状态$$h_t^{(e)}$$，可以用于下游的任务了。比如，我们可以计算下一个译句词的概率分布：
>
>$$
\begin{equation}
p_t^{(e)}= softmax(W_{hs}[h_t^{(e)};c_t]+b_s)
\end{equation}
>$$
>
>#### 8.4 计算注意得分的方式
>最后遗留的问题就是如何计算注意得分$$a_{t,j}$$了。有三种方式，各有各的优点：
> **点积:** 简单的计算点积来表示相似性：
>
>$$
\begin{equation}
attn\_score(h_j^{(f)},h_t^{(e)}) := h_j^{(f)T}h_t^{(e)}
\end{equation}
>$$
>
>这个方法的优点是不引入额外的参数，缺点是强制输入输出编码相同长度。
>
> **双线性函数：** 一个对于点积微小的更有表达力的改动是双线性函数。这个改动通过线性变换解除了源语言和目标语言的词嵌入必须在一个线性空间的限制：
>
>$$
\begin{equation}
attn\_score(h_j^{(f)},h_t^{(e)}) := h_j^{(f)T}W_ah_t^{(e)}
\end{equation}
>$$
>
>它引入了新的参数，可能不容易训练。
>
> **多层感知机:**
>
>$$
\begin{equation}
attn\_score(h_j^{(f)},h_t^{(e)}) :=w_{a2}^T tanh (W_{a1}[h_j^{(f)};h_t^{(e)}])
\end{equation}
>$$
>
>这个更灵活，参数比双线性函数少，表现也更好。
>
>这些都是最基本的计算注意得分的方式，还有更复杂的神经网络可用。
>#### 8.5 复制和位置词替代
>注意 有附加好处。一个是可以顺带进行次对齐，我们可以知道哪个词被翻译成哪个词。另一个是比较优雅地解决未知词。遇到未知词的时候，我们可以从目标词汇中找出有最大注意的词，这样就不需用用\<unk\>替代了。
