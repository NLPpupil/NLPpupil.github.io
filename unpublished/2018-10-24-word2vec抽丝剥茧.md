---
layout: post
title:  "word2vec抽丝剥茧"
date:   2018-10-24
categories: 自然语言处理 深度学习
---

如果你已经对word2vec有所了解，但看了很多论文和博客还是没有清楚弄明白，这篇就是为你而写的。
如果你刚开始了解word2vec，可以先去走一圈弯路，再回来看这篇。

本文借(chao)鉴(xi)了 [一篇通俗易懂的word2vec](https://zhuanlan.zhihu.com/p/35500923)，真的通俗易懂。然后在此基础上，进行了一些简化修改，又添加了部分内容。

那两篇经典论文就不贴了，开门见山。

#### CBOW
记词表大小为$V$，词向量大小为$N$。每个词由它的id表示，比如词$i$表示id为$i$的词。

训练样例是一个(window,center)对。如果center是词$t$，窗口大小为2，那么window由词$t-2$，词$t-1$，词$t+1$，词$t+2$四个词组成，即某个词的窗口由前两个词和后两个词组成。训练目标是通过窗口预测中心词，采用极大似然估计。

词向量矩阵记为$W_{V \times N}$，可视为一个lookup table，输入词id，输出相应词向量。词向量矩阵是模型参数，需要训练得来。用词向量矩阵得到窗口四个词每一个的词向量，然后取四个词向量的平均，作为窗口的特征向量。

特征向量经过一个分类矩阵 $W_{N \times V}^{'}$映射到长度为$V$的向量，分类矩阵是模型参数。这个向量的第$i$个元素是词$i$为预测词的几率(logit,表示是词$i$的概率除以不是词$i$的概率)。将此向量经过softmax得到概率分布，第$i$个元素表示窗口条件下词$i$为预测词的概率，即似然。最大化似然等效于最小化负对数似然，负对数似然的计算方式同交叉熵。将概率分布向量与center的独热向量计算交叉熵，就得到了损失。

训练模型最终需要的只是词向量矩阵，分类矩阵作辅助训练之用。


窗口词的顺序不重要，所以是bag-of-words。每个词的表示不是离散的独热向量，而是连续的词向量，所以叫continuous bag-of-words。

####  Continuous Skip Gram

与CBOW相反，训练样例变成(center,window)对。训练目标是通过中心词预测窗口的四个词。

首先用词向量矩阵$W_{V \times N}$获得中心词的向量，再用分类矩阵 $W_{N \times V}^{'}$加softmax获得预测概率分布向量。将预测概率分布复制四份，欲使其分别同时跟窗口词$t-2$，词$t-1$，词$t+1$，词$t+2$的独热向量靠近，即分别计算预测概率分布和四个窗口词的独热向量的交叉熵。取四个交叉熵的和作为损失。

#### 效率问题

CBOW中，窗口向量只取决于四个窗口词的词向量，损失对其参数求偏导的结果是只跟词向量矩阵中负责这四个向量的行有关。所以每次计算损失和更新梯度的时候，词向量矩阵需要更新四行。而分类层输出的几率向量是分类向量所有行同窗口向量分别内积的结果。所以每次计算损失和更新梯度的时候，需要更新全部词向量矩阵。

Skip Gram中，每次计算损失和更新梯度的时候，词向量矩阵需要更新负责中心词的那行，分类矩阵需要更新所有。





