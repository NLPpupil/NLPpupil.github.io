---
layout: post
title:  "word2vec抽丝剥茧"
date:   2018-10-24
categories: 自然语言处理 深度学习
---

如果你已经对word2vec有所了解，但看了很多论文和博客还是没有清楚弄明白，这篇就是为你而写的。
如果你刚开始了解word2vec，可以先去走一圈弯路，再回来看这篇。

虽然各种词向量包已经很成熟，可以直接拿来用，但是仔细研究一下word2vec对机器学习的理论和实践都有帮助。

那两篇经典论文就不贴了，开门见山。

#### CBOW
记词表大小为$V$，词向量大小为$N$。每个词由它的id表示，比如词$i$表示id为$i$的词。

训练样例是一个(window,center)对。如果center是词$t$，窗口大小为2，那么window由词$t-2$，词$t-1$，词$t+1$，词$t+2$四个词组成，即某个词的窗口由前两个词和后两个词组成。训练目标是通过窗口预测中心词，采用极大似然估计。

词向量矩阵记为$W_{V \times N}$，可视为一个lookup table，输入词id，输出相应词向量。词向量矩阵是模型参数，需要训练得来。用词向量矩阵得到窗口四个词每一个的词向量，然后取四个词向量的平均，作为窗口的特征向量。

特征向量经过一个分类矩阵 $W_{N \times V}^{'}$映射到长度为$V$的向量，分类矩阵是模型参数。这个向量的第$i$个元素是词$i$为预测词的几率(logit,表示是词$i$的概率除以不是词$i$的概率)。极大似然估计需要极大化center的似然，即极大化窗口词条件下center的概率。几率转化成概率需要经过一个softmax操作。极大化似然等效于极小化负对数似然，所以损失函数是center的概率的负对数。(也许有人会问，为什么不直接极小化负对数几率，而要先用softmax转化成概率？有两个原因，一是分类矩阵中对应某词的行的调整不是孤立的，既要使中心词的概率大，又要让别的词的概率小。二是极大似然是整个数据集的似然的极大化，不是单独某个样本的似然的极大化。如果极大化几率，各个样本的几率波动不统一，可能一个样本的几率是另一个的很多倍导致另一个样本可以忽略，为避免这个情况需要将几率归一化成概率。)

训练模型最终需要的只是词向量矩阵，分类矩阵作辅助训练之用。


窗口词的顺序不重要，所以是bag-of-words。每个词的表示不是离散的独热向量，而是连续的词向量，所以叫continuous bag-of-words。

####  Continuous Skip Gram

与CBOW相反，训练样例变成(center,window)对。训练目标是通过中心词预测窗口的四个词。

首先用词向量矩阵$W_{V \times N}$获得中心词的向量，再用分类矩阵 $W_{N \times V}^{'}$加softmax获得预测概率分布向量。将预测概率分布复制四份，欲使其分别同时跟窗口词$t-2$，词$t-1$，词$t+1$，词$t+2$的独热向量靠近，即分别计算四个窗口词的负概率，取四个负对数概率的和作为损失。

#### 效率问题
**参数更新效率**
CBOW中，窗口向量只取决于四个窗口词的词向量，损失对其参数求偏导的结果是只跟词向量矩阵中负责这四个向量的行有关。所以每次计算损失和更新梯度的时候，词向量矩阵需要更新四行。而分类层输出的几率向量是分类向量所有行同窗口向量分别内积的结果。所以每次计算损失和更新梯度的时候，需要更新全部分类矩阵。

Skip Gram中，每次计算损失和更新梯度的时候，词向量矩阵需要更新负责中心词的那行，分类矩阵需要更新所有。

**计算效率** CBOW中，要计算中心词的概率，首先要计算所有词的几率，然后softmax的时候涉及将$V$个词的几率的$e$的幂求和，时间复杂度是$O(V)$。Continuous Skip Gram同理。

#### 提高效率的方法
**Hierarchical Softmax** 先不要想Hierarchical Softmax是什么，Hierarchical Softmax替代了分类矩阵，起到了同样的将窗口向量映射到中心词概率的作用。

Hierarchical Softmax基于这样的思想：相比于直接建模$P(Y|X)$，我们可以先定义一个划分函数$c(\cdot)$将$Y$划分到区域$C$，然后：

$$
P(y \mid x) = P(y \mid c(y),X)P(c(y) \mid x)
$$

即计算$x$条件下$y$的概率，先计算$x$条件下$y$所在的区域的概率，再计算该区域条件下$y$的概率。这个方法可以嵌套，可以在$C$区域下再划分区域。

通过不断将样本空间(词汇表)分成两个大小相等的互补的集合，可以每次将样本空间的大小缩小一半，最终经过$\log_2(V)$步就可以得到想要的样本点。

具体的，将$V$个词作为均匀二叉树的最低层的叶。然后每相邻两个词划到一个区域，以倒数第二层的$V/2$个节点方式体现。再将倒数第二层的节点每相邻两个划到一个区域，以倒数第三层的$V/4$个节点体现。以此类推，树的跟代表了整个样本空间。一共有$V-1$个节点。每个节点都有一个跟窗口向量维度相同的向量，此向量的作用是对窗口向量划分区域。

对于某个特定的词，欲计算窗口条件下它出现的概率，先从叶逆向回溯出跟到此叶的路径。记窗口词的向量为$h$，从根开始逐行从左到右的节点为$V_1,V_2,...,V_{V-1}$。窗口向量从根开始沿着路径往下走，每遇到一个节点，就计算一次$\sigma(V_n^T  \cdot h)$，向量点击表示两个向量的相似度，经过一个sigmoid获得概率意义，如果在一个节点需要往左走，$P(c(y) \mid x)$就是$\sigma(V_n^T  \cdot h)$；如果需要往右走，$P(c(y) \mid x)$就是$1-\sigma(V_n^T  \cdot h)$。路径是固定的，那么条件概率链也是固定的。

每个区域的两个子区域都是该区域的一个分割，所以所有词的概率之和自动是$1$。



举个例子，假设有个句子是“我 女朋友 是 最 美 的 女人 。”。窗口长度是2，中心词是“美”的时候，窗口词是(是,最,的,女人)。通过词向量矩阵和求平均，可以得到窗口向量$h$。如图，红色是计算中心词概率的路径。$V_n$既表示相应位置的参数向量，也表示一个区域，有

$$
\begin{equation}
\begin{split}
P(美 \mid window) &= P(V_2 \mid window)\cdot P(V_5 \mid V_2)\cdot P(美 \mid V_5) \\
&= \sigma(V_1^T \cdot h)\cdot \sigma(1-V_2^T \cdot h) \cdot \sigma(V_5^T \cdot h)
\end{split} \tag 8
\end{equation}
$$

<img src="https://nlppupil.github.io/images/hie_soft.jpg" alt="BP" style="width:200px;height:380px;">

$V-1$个节点对应的$V-1$个向量是Hierarchical Softmax的参数，数量跟分类矩阵的$V$个向量几乎相同。不过，参数更新的时候，每次Hierarchical Softmax只需要更新路径上的$\log_2(V)$个向量，而分类矩阵需要更新全部向量。计算概率的时候，Hierarchical Softmax只需要路径上的$\log_2(V)$次计算，而普通分类矩阵+softmax的时间复杂度是$O(V)$。

**负采样Negative Sampling** 分类矩阵的第$i$行相当于一个权重，这个权重负责计算第$i$个词为中心词的分数(或几率)。分类矩阵+softmax的目的是使窗口向量尽可能地映射到中心词。可以想象，对于一个窗口向量和中心词，窗口向量经过分类矩阵后会有$V$个分数，代表每个词是预测词的几率，其中有的几率大，有的几率小。其中，中心词称作正样本，其他词称作负样本。在负样本中，也是有的几率大，有的几率小，如果只取少数几个几率大的负样本，不是可以近似估计正样本的概率了吗？从$V-1$个负样本中抽样少部分，计算量可以大大减少。

负采样就是基于这个知道思想来减少计算量。对窗口向量，中心词记为正样本，从某分布（比如均匀分布）中抽取$k$个样本作为负样本，然后用一个对数几率回归来分类正样本和负样本，这个对数几率分类器的参数就作为分类矩阵负责窗口词的那行。



